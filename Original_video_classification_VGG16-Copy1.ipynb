{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = r\"C:\\Users\\vedan\\Downloads\\data\\data\"\n",
    "outputmodel = r\"C:\\Users\\vedan\\Downloads\\data\\video_classification_model\\videoclassificationmodel_VGG19\"\n",
    "outputlabelbinarizer = r\"C:\\Users\\vedan\\Downloads\\data\\model\\videoclassificationoinarizerVGG19\"\n",
    "epoch = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images is being uploaded.....\n"
     ]
    }
   ],
   "source": [
    "sports_labels = set([\"basketball\", \"ballet\", \"dancehall\", \"hiphop\", \"house\", \"salsa\", \"jazz\", \"indianclassical\"])\n",
    "print(\"images is being uploaded.....\")\n",
    "pathToImages = list(paths.list_images(datapath))\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for images in pathToImages:\n",
    "    label = images.split(os.path.sep)[-2]\n",
    "    if label not in sports_labels:\n",
    "        continue\n",
    "    image = cv2.imread(images)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    data.append(image)\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.25, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAugmentation = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "validationAugmentation = ImageDataGenerator()\n",
    "mean = np.array([123.68, 110.779, 103.939], dtype=\"float32\")\n",
    "trainAugmentation.mean = mean\n",
    "validationAugmentation.mean = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications import ResNet50\n",
    "# from keras.applications import VGG19\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# baseModel = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224,244,3)))\n",
    "# baseModel = VGG19(include_top=False, weights='imagenet', input_tensor=Input(shape=(224,244,3)))\n",
    "baseModel = VGG16(include_top=False, weights='imagenet', input_tensor=Input(shape=(224,244,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 244, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 244, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 244, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 122, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 122, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 122, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 61, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 61, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 61, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 61, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 30, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 30, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 30, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 14,981,448\n",
      "Trainable params: 14,981,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7,7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(512, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "# headModel = Reshape((64, -1))(headModel)\n",
    "# # Just write this Permute after, its complicated why\n",
    "# headModel = Permute((2, 1))(headModel)\n",
    "# # it can also be an LSTM\n",
    "# headModel = SimpleRNN(128, activation='relu')(headModel)\n",
    "# headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(len(lb.classes_), activation=\"softmax\")(headModel)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "opt = SGD(lr = 0.0001, momentum=0.9, decay=1e-4/epoch)\n",
    "# https://keras.io/api/optimizers/sgd/\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "for basemodelLayers in baseModel.layers:\n",
    "    basemodelLayers.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr = 0.0001, momentum=0.9, decay=1e-4/epoch)\n",
    "# https://keras.io/api/optimizers/sgd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-ab54b713ea8a>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/25\n",
      "93/93 [==============================] - 330s 4s/step - loss: 3.9897 - accuracy: 0.3458 - val_loss: 1.1180 - val_accuracy: 0.6774\n",
      "Epoch 2/25\n",
      "93/93 [==============================] - 320s 3s/step - loss: 1.7699 - accuracy: 0.5910 - val_loss: 0.6873 - val_accuracy: 0.7762\n",
      "Epoch 3/25\n",
      "93/93 [==============================] - 323s 3s/step - loss: 1.2128 - accuracy: 0.6788 - val_loss: 0.5736 - val_accuracy: 0.8125\n",
      "Epoch 4/25\n",
      "93/93 [==============================] - 335s 4s/step - loss: 0.9869 - accuracy: 0.7177 - val_loss: 0.4126 - val_accuracy: 0.8700\n",
      "Epoch 5/25\n",
      "93/93 [==============================] - 384s 4s/step - loss: 0.7846 - accuracy: 0.7737 - val_loss: 0.3863 - val_accuracy: 0.8901\n",
      "Epoch 6/25\n",
      "93/93 [==============================] - 389s 4s/step - loss: 0.7382 - accuracy: 0.7835 - val_loss: 0.3289 - val_accuracy: 0.9073\n",
      "Epoch 7/25\n",
      "93/93 [==============================] - 381s 4s/step - loss: 0.6119 - accuracy: 0.8109 - val_loss: 0.3078 - val_accuracy: 0.9224\n",
      "Epoch 8/25\n",
      "93/93 [==============================] - 378s 4s/step - loss: 0.5553 - accuracy: 0.8278 - val_loss: 0.2818 - val_accuracy: 0.9234\n",
      "Epoch 9/25\n",
      "93/93 [==============================] - 347s 4s/step - loss: 0.5141 - accuracy: 0.8430 - val_loss: 0.2565 - val_accuracy: 0.9274\n",
      "Epoch 10/25\n",
      "93/93 [==============================] - 344s 4s/step - loss: 0.5020 - accuracy: 0.8467 - val_loss: 0.2496 - val_accuracy: 0.9375\n",
      "Epoch 11/25\n",
      "93/93 [==============================] - 345s 4s/step - loss: 0.4704 - accuracy: 0.8534 - val_loss: 0.2285 - val_accuracy: 0.9345\n",
      "Epoch 12/25\n",
      "93/93 [==============================] - 343s 4s/step - loss: 0.4244 - accuracy: 0.8588 - val_loss: 0.2218 - val_accuracy: 0.9395\n",
      "Epoch 13/25\n",
      "93/93 [==============================] - 342s 4s/step - loss: 0.3952 - accuracy: 0.8784 - val_loss: 0.2124 - val_accuracy: 0.9415\n",
      "Epoch 14/25\n",
      "93/93 [==============================] - 343s 4s/step - loss: 0.3839 - accuracy: 0.8723 - val_loss: 0.2067 - val_accuracy: 0.9385\n",
      "Epoch 15/25\n",
      "93/93 [==============================] - 344s 4s/step - loss: 0.3682 - accuracy: 0.8852 - val_loss: 0.1963 - val_accuracy: 0.9435\n",
      "Epoch 16/25\n",
      "93/93 [==============================] - 344s 4s/step - loss: 0.3668 - accuracy: 0.8892 - val_loss: 0.1864 - val_accuracy: 0.9476\n",
      "Epoch 17/25\n",
      "93/93 [==============================] - 344s 4s/step - loss: 0.3577 - accuracy: 0.8858 - val_loss: 0.1846 - val_accuracy: 0.9466\n",
      "Epoch 18/25\n",
      "93/93 [==============================] - 343s 4s/step - loss: 0.3107 - accuracy: 0.8956 - val_loss: 0.1818 - val_accuracy: 0.9516\n",
      "Epoch 19/25\n",
      "93/93 [==============================] - 344s 4s/step - loss: 0.3208 - accuracy: 0.8990 - val_loss: 0.1770 - val_accuracy: 0.9506\n",
      "Epoch 20/25\n",
      "93/93 [==============================] - 341s 4s/step - loss: 0.3003 - accuracy: 0.9034 - val_loss: 0.1691 - val_accuracy: 0.9526\n",
      "Epoch 21/25\n",
      "93/93 [==============================] - 340s 4s/step - loss: 0.3164 - accuracy: 0.8983 - val_loss: 0.1605 - val_accuracy: 0.9587\n",
      "Epoch 22/25\n",
      "93/93 [==============================] - 341s 4s/step - loss: 0.2911 - accuracy: 0.9102 - val_loss: 0.1599 - val_accuracy: 0.9556\n",
      "Epoch 23/25\n",
      "93/93 [==============================] - 343s 4s/step - loss: 0.2917 - accuracy: 0.9088 - val_loss: 0.1571 - val_accuracy: 0.9556\n",
      "Epoch 24/25\n",
      "93/93 [==============================] - 342s 4s/step - loss: 0.2464 - accuracy: 0.9189 - val_loss: 0.1445 - val_accuracy: 0.9577\n",
      "Epoch 25/25\n",
      "93/93 [==============================] - 341s 4s/step - loss: 0.2909 - accuracy: 0.9132 - val_loss: 0.1418 - val_accuracy: 0.9607\n"
     ]
    }
   ],
   "source": [
    "History = model.fit_generator(\n",
    "    trainAugmentation.flow(X_train, Y_train, batch_size=32),\n",
    "    steps_per_epoch=len(X_train) // 32,\n",
    "    validation_data=validationAugmentation.flow(X_test, Y_test),\n",
    "    validation_steps=len(X_test) // 32,\n",
    "    epochs=epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "outputmodel = r\"C:\\Users\\vedan\\Downloads\\data\\video_classification_model\\videoclassificationmodel\"\n",
    "# datapath = r\"C:\\Users\\vedan\\Downloads\\data\\data\"\n",
    "# outputmodel = r\"C:\\Users\\vedan\\Downloads\\data\\video_classification_model\\videoclassificationmodel_VGG19\"\n",
    "# outputlabelbinarizer = r\"C:\\Users\\vedan\\Downloads\\data\\model\\videoclassificationoinarizerVGG19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vedan\\anaconda3\\envs\\Project\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\vedan\\anaconda3\\envs\\Project\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\vedan\\Downloads\\data\\video_classification_model\\videoclassificationmodel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(outputmodel)\n",
    "lbinarizer = open(\"data_VGG16.pickle\", \"wb\")\n",
    "with open('data_VGG16.pickle', 'wb') as f:\n",
    "        pickle.dump(lb, f)\n",
    "# lbinarizer.write(pickle.dump(lb))\n",
    "lbinarizer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(outputmodel)\n",
    "with open('data_.pickle', 'rb') as f:\n",
    "    lb = pickle.load(f)\n",
    "# mean = np.array([123.68, 110.779, 103.939][::1], dtype=\"float32\")\n",
    "Queue = deque(maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queue = deque(maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputvideo = \"demo_output.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 224, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-18fc6b009ac5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQueue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"They are playing : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "capture_video = cv2.VideoCapture(\"test_video.mp4\")\n",
    "writer = None\n",
    "(Width, Height) = (None, None)\n",
    "\n",
    "while True:\n",
    "    (taken, frame) = capture_video.read()\n",
    "    if not taken:\n",
    "        break\n",
    "    if Width is None and Height is None:\n",
    "        (Width, Height) = frame.shape[:2]\n",
    "    \n",
    "    output = frame.copy()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (224, 244)).astype(\"float32\")\n",
    "#     print(frame.shape)\n",
    "#     frame = mean\n",
    "    print(frame.shape)\n",
    "    preds = model.predict(np.expand_dims(frame, axis=0))[0]\n",
    "    Queue.append(preds)\n",
    "    results = np.array(Queue).mean(axis=0)\n",
    "    i = np.argmax(results)\n",
    "    label = lb.classes[i]\n",
    "    text = \"They are playing : {}\".format(label)\n",
    "    cv2.putText(output, text, (45, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (255,0,0), 5)\n",
    "    \n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    writer = cv2.VideoWriter(\"outputvideo\", fourcc, 30, (Width, Height), True)\n",
    "    cv2.imshow(\"In progress\" ,output)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "print(\"Finalizing.....\")\n",
    "writer.release()\n",
    "capture_video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images is being uploaded.....\n",
      "(97, 244, 224, 3)\n",
      "(73, 244, 224, 3)\n",
      "73\n",
      "73\n",
      "Evaluate on test data\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 244, 3) for input Tensor(\"input_2:0\", shape=(None, 224, 244, 3), dtype=float32), but it was called on an input with incompatible shape (None, 244, 224, 3).\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.1748 - accuracy: 0.9315\n",
      "test loss, test acc: [0.1748356819152832, 0.931506872177124]\n"
     ]
    }
   ],
   "source": [
    "datapath = r\"C:\\Users\\vedan\\Downloads\\data\\data\\test_data\"\n",
    "\n",
    "sports_labels = set([\"basketball\", \"ballet\", \"dancehall\", \"hiphop\", \"house\", \"salsa\", \"jazz\", \"indianclassical\"])\n",
    "print(\"images is being uploaded.....\")\n",
    "pathToImages = list(paths.list_images(datapath))\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for images in pathToImages:\n",
    "    label = images.split(os.path.sep)[-2]\n",
    "    if label not in sports_labels:\n",
    "        continue\n",
    "    image = cv2.imread(images)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 244))\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.75, stratify=labels, random_state=42)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(Y_test))\n",
    "\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f5e6efd1ad3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcxklEQVR4nO3debRedX3v8feHSQQCBBMsJYQEiyKiAj4XsHAVHDCyJGCxCKICUtKK4ECvq3h7b6Gw1r1UilZaplQioAIqDuQig1SByBDNCUSGKBLDlBQNEiBh1ITP/WPv0zwczrDP5uxznifn81rrWefZvz19s1dyvvnt3yTbREREDNcGYx1ARER0pySQiIioJQkkIiJqSQKJiIhakkAiIqKWJJCIiKilsQQiaQdJN0paLOleSZ/p5xhJOkfSEkl3Sdqzbd/Rku4vP0c3FWdERNSjpsaBSNoO2M72HZImAAuBQ20vbjvmIOAk4CBgb+ArtveWtA3QA7QAl+e+zfYTjQQbERHD1lgNxPajtu8ov68Gfgls3+ewQ4BLXZgPbF0mnvcBN9heWSaNG4AZTcUaERHDt9Fo3ETSNGAP4Gd9dm0PPNK2vawsG6i8v2vPAmYBbL755m/bZZddRiboiIhxYOHChb+3PbnOuY0nEElbAN8FPmt71Uhf3/ZsYDZAq9VyT0/PSN8iImK9Jemhuuc22gtL0sYUyeObtr/XzyHLgR3atqeUZQOVR0REh2iyF5aAi4Bf2v7SAIfNBT5e9sbaB3jK9qPA9cCBkiZKmggcWJZFRESHaPIV1r7Ax4C7JS0qy/4nMBXA9gXANRQ9sJYAzwLHlvtWSjoDWFCed7rtlQ3GGhERw9RYArF9C6AhjjHwqQH2zQHmNBBaRESMgIxEj4iIWpJAIiKiliSQiIioJQkkIiJqSQKJiIhakkAiIqKWJJCIiKglCSQiImpJAomIiFqSQCIiopYkkIiIqCUJJCIiakkCiYiIWpJAIiKiliSQiIioJQkkIiJqSQKJiIhakkAiIqKWxpa0lTQH+ACwwvZu/ez/PHBUWxxvBCaX66E/CKwG1gJrbLeaijMiIuppsgZyMTBjoJ22z7K9u+3dgS8AN9te2XbIAeX+JI+IiA7UWAKxPQ9YOeSBhSOBy5uKJSIiRt6Yt4FI2oyipvLdtmIDP5K0UNKssYksIiIG01gbyDAcDNza5/XVfraXS9oWuEHSr8oazcuUCWYWwNSpU5uPNiIigA6ogQBH0Of1le3l5c8VwPeBvQY62fZs2y3brcmTJzcaaERErDOmCUTSVsA7gavayjaXNKH3O3AgcM/YRBgREQNpshvv5cD+wCRJy4BTgY0BbF9QHvZB4Ee2n2k79bXA9yX1xneZ7euaijMiIuppLIHYPrLCMRdTdPdtL1sKvLWZqCIiYqR0QhtIRER0oSSQiIioJQkkIiJqSQKJiIhakkAiIqKWIXthlaPB9wX+FHiOYkxGj+0XG44tIiI62IAJRNIBwCnANsCdwApgU+BQ4HWSrgTOtr1qFOKMiIgOM1gN5CDgeNsP990haSOKtT7ey0snQYyIiHFiwARi+/OD7FsD/KCJgCIiojtUaQN5FXAYMK39eNunNxdWRER0uipTmVwFPAUsBF5oNpyIiOgWVRLIFNsDLk0bERHjU5VxILdJenPjkURERFepUgPZDzhG0gMUr7AE2PZbGo0sIiI6WpUE8v7Go4iIiK4z5Css2w8BW1OsXX4wsHVZFhER49iQCUTSZ4BvAtuWn29IOqnpwCIiorNVeYV1HLB377Kzkv4JuB341yYDi4iIzlalF5aAtW3ba8uyiIgYx6okkK8BP5N0mqTTgPnARUOdJGmOpBWS7hlg//6SnpK0qPz8Q9u+GZLuk7RE0ikV/ywRETGKhnyFZftLkm6i6M4LcKztOytc+2Lg34BLBznmp7Y/0F4gaUPgXIqJGpcBCyTNtb24wj0jImKUDDad+5a2V0naBniw/PTu28b2ysEubHuepGk1YtoLWGJ7aXmvK4BDgCSQiIgOMlgN5DKKKdsXAm4rV7m90wjc/+2SfgH8J/A/bN8LbA880nbMMmDvgS4gaRYwC2Dq1KkjEFJERFQx2HTuHyh/Tm/o3ncAO9p+WtJBFNPD7zzci9ieDcwGaLVaHuLwiIgYIVXGgfy4Stlw2V5l++ny+zXAxpImAcuBHdoOnVKWRUREBxmsDWRTYDNgkqSJrOu6uyXFa6ZXRNKfAL+zbUl7USSzx4EngZ0lTadIHEcAH3ml94uIiJE1WBvIXwOfBf6Uoh2kN4GsouhdNShJlwP7UySgZcCpwMYAti8APgR8UtIa4DngCNsG1kg6Ebge2BCYU7aNREREB1HxO3uQA6STbHfFqPNWq+Wenp6xDiMiomtIWmi7VefcKuNA/lXSbsCuwKZt5YON74iIiPVclTXRT6V4FbUrcA3F9O63MPgAwYiIWM9VmcrkQ8C7gd/aPhZ4K7BVo1FFRETHq5JAnrP9IkXj9pbACl7azTYiIsahKtO590jaGvh3it5YT1NM5x4REeNYlUb0E8qvF0i6DtjS9l3NhhUREZ1usIGEew62z/YdzYQUERHdYLAayNnlz02BFvALisGEbwF6gLc3G1pERHSyARvRbR9g+wDgUWBP2y3bbwP2IHNTRUSMe1V6Yb3B9t29G7bvAd7YXEgREdENqvTCukvSV4FvlNtHAWlEj4gY56okkGOBTwKfKbfnAec3FlFERHSFKt14nwe+XH4iIiKAwbvxftv24ZLu5qVL2gJg+y2NRhYRER1tsBpI7yurD4xGIBER0V0GWxP90fLnQ6MXTkREdIvBXmGtpp9XVxSDCW17y8aiioiIjjdYDWTCaAYSERHdpcpAQgAkbStpau+nwvFzJK2QdM8A+4+SdJekuyXdJumtbfseLMsXScoatRERHWjIBCJppqT7gQeAm4EHgWsrXPtiYMYg+x8A3mn7zcAZwOw++w+wvXvdtXojIqJZVWogZwD7AL+2PZ1idcL5Q51kex6wcpD9t9l+otycD0ypEEtERHSIKgnkj7YfBzaQtIHtGylm5x1Jx/HSWo2BH0laKGnWYCdKmiWpR1LPY489NsJhRUTEQKpMZfKkpC0opjD5pqQVwDMjFYCkAygSyH5txfvZXi5pW+AGSb8qazQvY3s25euvVqvVX6+xiIhoQJUayCHAs8DngOuA3wAHj8TNJb0F+CpwSFnLAcD28vLnCuD7wF4jcb+IiBg5VRLIXwPb2V5j+xLb57T/sq+r7Mn1PeBjtn/dVr65pAm934EDgX57ckVExNip8gprAkV7xErgW8B3bP9uqJMkXQ7sD0yStAw4FdgYwPYFwD8ArwHOkwSwpuxx9Vrg+2XZRsBltq8b5p8rIiIaJrtas0H5uunDwGHAMtvvaTKwOlqtlnt6MmwkIqIqSQvrDpeoPJAQWAH8Fngc2LbOzSIiYv1RZSDhCZJuAn5M8crp+EzlHhERVdpAdgA+a3tRw7FEREQXqbIi4RdGI5CIiOguw2kDiYiI+C9JIBERUUsSSERE1FJnRUIAsiJhRMT4NuSKhJLOAB4Fvk6xnO1RwHajEl1ERHSsKq+wZto+z/Zq26tsn08xwWJERIxjVRLIM+XysxtK2kDSUYzgdO4REdGdqiSQjwCHA78rP39ZlkVExDhWZSDhg+SVVURE9DFkApE0GTgemNZ+vO1PNBdWRER0uipzYV0F/BT4D2Bts+FERES3qJJANrP9d41HEhERXaVKI/rVkg5qPJKIiOgqVRLIZyiSyHOSVklaLWlV04FFRERnGzKB2J5gewPbr7a9ZbldaRoTSXMkrZB0zwD7JekcSUsk3SVpz7Z9R0u6v/wcXf2PFBERo6FKGwiSJgI7A5v2ltmeV+HUi4F/Ay4dYP/7y+vuDOwNnA/sLWkb4FSgRTEf10JJc20/USXeiIhoXpVuvH9F8RprCrAI2Ae4HXjXUOfanidp2iCHHAJcatvAfElbS9oO2B+4wfbKMoYbgBnA5UPdMyIiRkfVNpD/Bjxk+wBgD+DJEbr/9sAjbdvLyrKByl9G0ixJPZJ6HnvssREKKyIihlIlgTxv+3kASa+y/SvgDc2GVZ3t2bZbtluTJ08e63AiIsaNKglkmaStgR8AN0i6CnhohO6/HNihbXtKWTZQeUREdIgqvbA+aPtJ26cB/xu4CDh0hO4/F/h42RtrH+Ap248C1wMHSppYNuAfWJZFRESHqNQLq5ftm4dzvKTLKRrEJ0laRtGzauPyWhcA1wAHAUuAZ4Fjy30ry4WsFpSXOr23QT0iIjrDsBLIcNk+coj9Bj41wL45wJwm4oqIiFeuShtIRETEyySBRERELVUGEq6mGA3e7imgB/hb20ubCCwiIjpblTaQf6EYyHcZIOAI4HXAHRRtFPs3FFtERHSwKq+wZtq+0PZq26tszwbeZ/tbwMSG44uIiA5VJYE8K+lwSRuUn8OB58t9fV9tRUTEOFElgRwFfAxYAfyu/P5RSa8GTmwwtoiI6GBDtoGUjeQHD7D7lpENJyIiukWVXliTgeOBae3H2/5Ec2FFRESnq9IL6yrgp8B/AGubDSciIrpFlQSyme2/azySiIjoKlUa0a+WdFDjkURERFepuiLh1ZKek7RK0mpJq5oOLCIiOluVXlgTRiOQiIjoLgMmEEm72P6VpD3722/7jubCioiITjdYDeRkYBZwdj/7DLyrkYgiIqIrDJhAbM8qfx4weuFERES3qLQioaQ/5+UDCS9tKKaIiOgCVUaif51i+vZFrBtIaGDIBCJpBvAVYEPgq7bP7LP/y0BvDWczYFvbW5f71gJ3l/setj1zqPtFRMToqVIDaQG7luuXVyZpQ+Bc4L0U64kskDTX9uLeY2x/ru34k4A92i7xnO3dh3PPiIgYPVXGgdwD/EmNa+8FLLG91PYfgCuAQwY5/kjg8hr3iYiIMVClBjIJWCzp58ALvYUVXiltDzzStr0M2Lu/AyXtCEwHftJWvKmkHmANcKbtHwxw7iyK3mJMnTp1iJAiImKkVEkgpzUdBMUyuVfabp+scUfbyyXtBPxE0t22f9P3xHKFxNkArVYrC1xFRIySKiPRb6557eXADm3bU8qy/hwBfKrPfZeXP5dKuomifeRlCSQiIsbGgG0gkm4pf64u58BaNcy5sBYAO0uaLmkTiiQxt5/77EKxtvrtbWUTJb2q/D4J2BdY3PfciIgYO4MNJNyv/FlrLizbaySdCFxP0Y13ju17JZ0O9NjuTSZHAFf06eX1RuBCSS9SJLkz23tvRUTE2FPV3rmStgU27d22/XBTQdXVarXc09Mz1mFERHQNSQttt+qcO2Q3XkkzJd0PPADcDDwIXFvnZhERsf6oMg7kDGAf4Ne2pwPvBuY3GlVERHS8Kgnkj7YfBzaQtIHtGylGp0dExDhWZRzIk5K2AOYB35S0Anim2bAiIqLTVamBHAI8C3wOuI5iLMbBTQYVERGdb9AaSDkh4tXlmiAvApeMSlQREdHxBq2BlFOLvChpq1GKJyIiukSVNpCngbsl3UBb24ftTzcWVUREdLwqCeR75addJi2MiBjnqiSQrW1/pb1A0mcaiiciIrpElV5YR/dTdswIxxEREV1mwBqIpCOBjwDTJbXPojsBWNl0YBER0dkGe4V1G/AoxYqEZ7eVrwbuajKoiIjofIMlkIdtPwS8faADJMlVp/ONiIj1ymBtIDdKOknSSxYal7SJpHdJuoT+20ciImIcGKwGMgP4BHC5pOnAk8CrKZLOj4B/sX1n4xFGRERHGmxFwueB84DzJG1M0RbynO0nRym2iIjoYFXGgWD7jxQN6hEREUC1cSC1SZoh6T5JSySd0s/+YyQ9JmlR+fmrtn1HS7q//KStJSKiw1SqgdRRzuR7LvBeYBmwQNJc24v7HPot2yf2OXcb4FSKhasMLCzPfaKpeCMiYniqrIm+uaQNyu+vL9dI37jCtfcCltheavsPwBUUa4tU8T7gBtsry6RxA0WjfkREdIgqr7DmAZtK2p6i99XHgIsrnLc98Ejb9rKyrK/DJN0l6UpJOwzzXCTNktQjqeexxx6rEFZERIyEKglEtp8F/gI4z/ZfAm8aofv/P2Ca7bdQ1DKGvWCV7dm2W7ZbkydPHqGwIiJiKJUSiKS3A0cBPyzLNqxw3nJgh7btKWXZf7H9uO0Xys2vAm+rem5ERIytKgnks8AXgO/bvlfSTsCNFc5bAOwsabqkTYAjgPZJGZG0XdvmTOCX5ffrgQMlTZQ0ETiwLIuIiA4xZC8s2zcDNwOUjem/r7Iaoe01kk6k+MW/ITCnTECnAz225wKfljQTWEMxw+8x5bkrJZ1BkYQATredGYAjIjqIhpoLUdJlwN8Aayl+oW8JfMX2Wc2HNzytVss9PT1jHUZERNeQtNB2q865VV5h7Wp7FXAocC0wnaInVkREjGNVEsjG5biPQ4G55bQmmcI9ImKcq5JALgQeBDYH5knaEVjVZFAREdH5qjSinwOc01b0kKQDmgspIiK6QZWpTLaS9KXe0d6SzqaojURExDhW5RXWHIp10A8vP6uArzUZVEREdL4qs/G+zvZhbdv/KGlRQ/FERESXqFIDeU7Sfr0bkvYFnmsupIiI6AZVaiB/A1wqaaty+wkgCzxFRIxzVXph/QJ4q6Qty+1Vkj4L3NVwbBER0cEqL2lre1U5Ih3g5IbiiYiILlF3TXSNaBQREdF16iaQTGUSETHODdgGImk1/ScKAa9uLKKIiOgKAyYQ2xNGM5CIiOgudV9hRUTEOJcEEhERtSSBRERELY0mEEkzJN0naYmkU/rZf7KkxZLukvTjcq2R3n1rJS0qP3ObjDMiIoavylQmtUjaEDgXeC+wDFggaa7txW2H3Qm0bD8r6ZPAF4EPl/ues717U/FFRMQr02QNZC9gie2ltv8AXAEc0n6A7RttP1tuzgemNBhPRESMoCYTyPbAI23by8qygRwHXNu2vWm5gNV8SYc2EF9ERLwCjb3CGg5JHwVawDvbine0vVzSTsBPJN1t+zf9nDsLmAUwderUUYk3IiKarYEsB3Zo255Slr2EpPcAfw/MtP1Cb7nt5eXPpcBNwB793cT2bNst263JkyePXPQRETGoJhPIAmBnSdMlbQIcAbykN5WkPYALKZLHirbyiZJeVX6fBOwLtDe+R0TEGGvsFZbtNZJOBK4HNgTm2L5X0ulAj+25wFnAFsB3JAE8bHsm8EbgQkkvUiS5M/v03oqIiDEme/2ZWLfVarmnp2esw4iI6BqSFtpu1Tk3I9EjIqKWJJCIiKglCSQiImpJAomIiFqSQCIiopYkkIiIqCUJJCIiakkCiYiIWpJAIiKiliSQiIioJQkkIiJqSQKJiIhakkAiIqKWJJCIiKglCSQiImpJAomIiFqSQCIiopYkkIiIqCUJJCIiamk0gUiaIek+SUskndLP/ldJ+la5/2eSprXt+0JZfp+k9zUZZ0REDF9jCUTShsC5wPuBXYEjJe3a57DjgCds/xnwZeCfynN3BY4A3gTMAM4rrxcRER2iyRrIXsAS20tt/wG4AjikzzGHAJeU368E3i1JZfkVtl+w/QCwpLxeRER0iI0avPb2wCNt28uAvQc6xvYaSU8BrynL5/c5d/v+biJpFjCr3HxB0j2vPPT1wiTg92MdRAfIc1gnz2KdPIt13lD3xCYTyKiwPRuYDSCpx3ZrjEPqCHkWhTyHdfIs1smzWEdST91zm3yFtRzYoW17SlnW7zGSNgK2Ah6veG5ERIyhJhPIAmBnSdMlbULRKD63zzFzgaPL7x8CfmLbZfkRZS+t6cDOwM8bjDUiIoapsVdYZZvGicD1wIbAHNv3Sjod6LE9F7gI+LqkJcBKiiRDedy3gcXAGuBTttdWuO3sJv4sXSrPopDnsE6exTp5FuvUfhYq/sMfERExPBmJHhERtSSBRERELV2XQF7J9CjrmwrP4mRJiyXdJenHknYcizhHw1DPou24wyRZ0nrbhbPKs5B0ePl3415Jl412jKOlwr+RqZJulHRn+e/koLGIczRImiNpxUBj5VQ4p3xWd0nac8iL2u6aD0Vj/G+AnYBNgF8Au/Y55gTggvL7EcC3xjruMXwWBwCbld8/OZ6fRXncBGAexSDV1ljHPYZ/L3YG7gQmltvbjnXcY/gsZgOfLL/vCjw41nE3+DzeAewJ3DPA/oOAawEB+wA/G+qa3VYDeSXTo6xvhnwWtm+0/Wy5OZ9iPM36qMrfC4AzKOZbe340gxtlVZ7F8cC5tp8AsL1ilGMcLVWehYEty+9bAf85ivGNKtvzKHq7DuQQ4FIX5gNbS9pusGt2WwLpb3qUvlOcvGR6FKB3epT1TZVn0e44iv9drI+GfBZldXwH2z8czcDGQJW/F68HXi/pVknzJc0YtehGV5VncRrwUUnLgGuAk0YntI403N8p3T+VSQxN0keBFvDOsY5lLEjaAPgScMwYh9IpNqJ4jbU/Ra10nqQ3235yLIMaI0cCF9s+W9LbKcal7Wb7xbEOrBt0Ww3klUyPsr6pNN2LpPcAfw/MtP3CKMU22oZ6FhOA3YCbJD1I8X537nrakF7l78UyYK7tP7qY7frXFAllfVPlWRwHfBvA9u3AphQTLY5Hw55CqtsSyCuZHmV9M+SzkLQHcCFF8lhf33PDEM/C9lO2J9meZnsaRXvQTNu1J5HrYFX+jfyAovaBpEkUr7SWjmKMo6XKs3gYeDeApDdSJJDHRjXKzjEX+HjZG2sf4Cnbjw52Qle9wvIrmB5lfVPxWZwFbAF8p+xH8LDtmWMWdEMqPotxoeKzuB44UNJiYC3wedvrXS294rP4W+DfJX2OokH9mPX0P5xIupziPw6TyjafU4GNAWxfQNEGdBDF+kvPAscOec319FlFRETDuu0VVkREdIgkkIiIqCUJJCIiakkCiYiIWpJAIiKiliSQWG9Jeo2kReXnt5KWt21vMsS5LUnnVLjHbSMU62aSvinpbkn3SLpF0haStpZ0wkjcI2KkpRtvjAuSTgOetv3PbWUblfOljTlJXwAm2z653H4D8CCwHXC17d3GMLyIfqUGEuOKpIslXSDpZ8AXJe0l6fZyPYjbyl/cSNpf0tXl99PKtRRukrRU0qfbrvd02/E3SbpS0q/K2oTKfQeVZQvL9Rau7ie07WibNsL2feXUM2cCrytrTWeV1/u8pAXlmg3/WJZNa7vvL8s4Niv3nal168L8cz/3jqilq0aiR4yQKcCf214raUvgv5ejlt8D/B/gsH7O2YVifZUJwH2Szrf9xz7H7AG8iWJK8FuBfSX1UEwn8w7bD5SjgfszB/iRpA8BPwYusX0/cAqwm+3dASQdSDFv1V4U6zbMlfQOiik53gAcZ/tWSXOAEyR9DfggsIttS9p6mM8qYkCpgcR49B3ba8vvW1FM9XIP8GWKBNCfH9p+wfbvgRXAa/s55ue2l5UzuS4CplEknqXlpIUA/SYQ24soFj46C9gGWFDOzdTXgeXnTuCO8vq9EyE+YvvW8vs3gP0oljN4HrhI0l9QTFERMSKSQGI8eqbt+xnAjWUbw8EUk+n1p30m47X0X3uvcsyAbD9t+3u2T6BIAP0tryrg/9revfz8me2Lei/x8kt6DUVt5UrgA8B1w4kpYjBJIDHebcW6todjGrj+fcBOkqaV2x/u7yBJ+0qaWH7fhGJ51YeA1RSvzXpdD3xC0hblsdtL2rbcN1XFmhYAHwFuKY/byvY1wOeAt47YnyzGvbSBxHj3ReASSf8LGPHVCm0/V3bDvU7SMxRTjPfndcD5ZcP7BmUs3y3bLW4tX7Fda/vz5aut28s2+qeBj1LUeO4DPlW2fywGzqdIkFdJ2pSi9nLySP8ZY/xKN96IhknawvbTZXI4F7jf9pdH+B7TSHffGGV5hRXRvOMlLQLupagRXDi24USMjNRAIiKiltRAIiKiliSQiIioJQkkIiJqSQKJiIhakkAiIqKW/w94747trVRfmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,2])\n",
    "plt.plot(history[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(hist[\"accuracy\"])\n",
    "plt.plot(hist[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture_video = cv2.VideoCapture(\"demo\")\n",
    "\n",
    "# cv2.imshow(\"In progress\", capture_video)\n",
    "# key = cv2.waitkey(1) & oxFF\n",
    "    \n",
    "# if key == ord(\"q\"):\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Create a VideoCapture object and read from input file\n",
    "# # If the input is the camera, pass 0 instead of the video file name\n",
    "# cap = cv2.VideoCapture('demo.mp4')\n",
    "\n",
    "# (taken, frame) = cap.read()\n",
    "# print(frame.shape)\n",
    "# # Check if camera opened successfully\n",
    "# if (cap.isOpened()== False): \n",
    "#   print(\"Error opening video stream or file\")\n",
    "\n",
    "# # Read until video is completed\n",
    "# while(cap.isOpened()):\n",
    "#   # Capture frame-by-frame\n",
    "#   ret, frame = cap.read()\n",
    "#   if ret == True:\n",
    "\n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('Frame',frame)\n",
    "\n",
    "#     # Press Q on keyboard to  exit\n",
    "#     if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#       break\n",
    "\n",
    "#   # Break the loop\n",
    "#   else: \n",
    "#     break\n",
    "\n",
    "# # When everything done, release the video capture object\n",
    "# cap.release()\n",
    "\n",
    "# # Closes all the frames\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
